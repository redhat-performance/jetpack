{% if clean_nodes != true %}
resource_registry:
  OS::TripleO::NodeUserData: /home/stack/wipe-disks.yaml
{% endif %}

parameter_defaults:
  LocalCephAnsibleFetchDirectoryBackup: /tmp/fetch_dir
{% if ceph_node_count < 3 %}
  CephPoolDefaultSize: {{ ceph_node_count }}
{% else %}
CephPoolDefaultSize: 3
{% endif %}
  # when deploying a small number of osd's - < 12), it's necessary to decrease the default pg_num from 128 to get past the max 200pgs/per osd  limitation
  CephPoolDefaultPgNum: 32
  CephAnsiblePlaybookVerbosity: 1
  CephAnsibleDisksConfig:
    devices:
{% for disk in storage_node_disks[1:] %}
      - {{ disk }}
{% endfor %}

# the following two parameters are the defaults. Just included them here for info
    osd_scenario: {{ osd_scenario }}
    osd_objectstore: {{ osd_objectstore }}
  CephAnsibleExtraConfig:
    osd_pool_default_autoscale_mode: on

  ExtraConfig:
    ceph::profile::params::osd_pool_default_pg_num: {{ osd_pool_default_pg_num | default(32) }}
    ceph::profile::params::osd_pool_default_pgp_num: {{ osd_pool_default_pgp_num | default(32) }}
