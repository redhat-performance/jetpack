---
infrared_dir: "{{ ansible_user_dir }}/infrared"
infrared_workspaces_dir: "{{ ansible_user_dir }}/.infrared/.workspaces"
infrared_hosts_file: "{{ infrared_workspaces_dir }}/active/hosts"
instackenv_file: "{{ playbook_dir }}/instackenv.json"
undercloud_conf: "{{ ansible_user_dir }}/undercloud.conf"
log_directory: "{{ playbook_dir }}/jetpack/logs"
cloud_name: cloud00
lab_name: beaker
ansible_ssh_pass: "{{ lookup('env', 'ANSIBLE_SSH_PASSWORD')|default('', true) }}"
ansible_ssh_key: "{{ ansible_user_dir }}/.ssh/id_rsa"
osp_release: 13
osp_puddle: passed_phase2
baseurl: http://download.eng.pek2.redhat.com/released/RHEL-7/7.9/Server/x86_64/os/
controller_count: 1
# No need to set compute_count. This will be set to all remaining nodes, which is calclulated in overcloud.yml
compute_count: 1
ceph_node_count: 0
set_boot_order: false
hammer_host: hwstore.example.com
alias:
#lab specific vars
  lab_url: "http://quads.alias.bos.scalelab.redhat.com"
  machine_types:
    dell:
      740xd:
        rhel7_interfaces: [em1, em2, p7p1, p7p2]
        rhel8_interfaces: []
    supermicro:
      6029r:
        rhel7_interfaces: [enp95s0f0, enp95s0f1, enp94s0f0, enp94s0f1]
        rhel8_interfaces: []
      6048p:
        rhel7_interfaces: [enp130s0f0, enp130s0f1, enp4s0f0, enp4s0f1]
        rhel8_interfaces: []

scale:
# lab specific vars
  lab_url: "http://quads.rdu2.scalelab.redhat.com"
  machine_types:
    supermicro:
      1029p:
        rhel7_interfaces: [enp94s0f0, enp94s0f1, enp94s0f2, enp94s0f3]
        rhel8_interfaces: [ens2f0, ens2f1, ens2f2, ens2f3]
      1029u:
        rhel7_interfaces: [enp175s0f0, enp175s0f1, enp216s0f0, enp216s0f1]
        rhel8_interfaces: [ens1f0, ens1f1, ens2f0, ens2f1 ]
      6048r:
        rhel7_interfaces: [enp4s0f0, enp4s0f1, enp131s0f0, enp131s0f1]
        rhel8_interfaces: []
      5039ms:
        rhel7_interfaces:  [enp1s0f0, enp1s0f1, enp2s0f1]
        rhel8_interfaces:  [enp1s0f0, enp1s0f1, enp2s0f1]
      6049p:
        rhel7_interfaces: [enp175s0f0, enp175s0f1, enp216s0f0, enp216s0f1]
        rhel8_interfaces: [enp175s0f0, enp175s0f1, enp216s0f0, enp216s0f1]
      6048p:
        rhel7_interfaces:  []
        rhel8_interfaces:  []

    dell:
      r620:
        rhel7_interfaces: [p2p3, p2p4, em1, em2]
        rhel8_interfaces: [enp66s0f2, enp66s0f3, eno1, eno2]
      r630:
        rhel7_interfaces: [em1, em2, em3, em4]
        rhel8_interfaces: [eno1, eno2, eno3, eno4]
      r730xd:
        rhel7_interfaces: [em1, em2, p4p1, p4p2]
        rhel8_interfaces: []
      r930:
        rhel7_interfaces: [em1, em2, p1p1, p1p2]
        rhel8_interfaces: []

# Other lab machines have to define their interfaces through below
# 'interfaces' variable. prepare_nic_configs.yml will use that
# instead of deriving from
# scale/alias.machine_types.supermicro/dell...interfaces
# note: If you are using scale lab or alias lab, don't enable below
# 'interfaces' parameter.
interfaces:
  rhel8_interfaces: [eno1]
  rhel7_interfaces: [enp1s0f1]

#extra_templates:

# containers params
registry_mirror: redhat.com
registry_namespace: redhat
insecure_registries: redhat.com
# undercloud.conf ctlplane-subnet section config options
cidr: 192.168.24.0/24
gateway: 192.168.24.1
dhcp_start: 192.168.24.5
dhcp_end: 192.168.24.105
inspection_iprange: 192.168.24.110,192.168.24.250
# external network params for adding external network to
# undercloud to access overcloud resources
external_gateway: 172.17.5.1/24
external_network_vlan_id: 300
clean_nodes: False
#adding changes 
external_net_cidr: 172.17.5.0/24
external_allocation_pools_start: 172.17.5.50
external_allocation_pools_end: 172.17.5.150
external_interface_default_route: 172.17.5.1
#internal
internal_api_net_cidr: 172.17.1.0/24
internal_api_allocation_pools_start: 172.17.1.10
internal_api_allocation_pools_end: 172.17.1.149
internal_api_network_vlan_id: 301
#storage
storage_net_cidr: 172.17.3.0/24
storage_allocation_pools_start: 172.17.3.10
storage_allocation_pools_end: 172.17.3.149
storage_network_vlan_id: 302
storage_mgmt_net_cidr: 172.17.4.0/24
storage_mgmt_allocation_pools_start: 172.17.4.10
storage_mgmt_allocation_pools_end: 172.17.4.149
storage_mgmt_network_vlan_id: 303
#tenant
tenant_net_cidr: 172.17.2.0/24
tenant_allocation_pools_start: 172.17.2.10
tenant_allocation_pools_end: 172.17.2.150
tenant_network_vlan_id: 304
#This allows the user to force re provision undercloud
#default is false - not forced
force_reprovision: false
#This enables user to have a undercloud in VM
#NOTE: now the vm is created in the ansible host, so
#do not enable this while running from your desktop
virtual_uc: true
undercloud_host: 172.16.0.2
vm_external_interface: eth0
undercloud_local_interface: eth0
virtual_uc_ctlplane_interface: enp1s0f1
vm_image_url:
   rhel7: http://download.eng.pek2.redhat.com/released/RHEL-7/7.9/Server/x86_64/images/rhel-guest-image-7.9-30.x86_64.qcow2
   rhel8: http://download.hosts.prod.upshift.rdu2.redhat.com/released/RHEL-8/8.1.0/BaseOS/x86_64/images/rhel-guest-image-8.1-263.x86_64.qcow2
   rhel8_2: http://download.eng.pek2.redhat.com/released/RHEL-8/8.2.0/BaseOS/x86_64/images/rhel-guest-image-8.2-290.x86_64.qcow2
#Neutron backend, if not set default will be used.
#neutron_backend: ovn
#ocp installation
shift_stack: false

# define mount_nvme to mount /var/lib/nova on nvme device
# to use nvme storage for nova instances as ephemeral disk
#mount_nvme: true

# enable nvme parameters
# README has details about how to get these values
#passthrough_nvme:
#    vendor_id: '144d'
#    product_id: 'a804'
#    address: '01:00.0'
#    flavor:
#      name: 'nvme'
#      ram: 16384
#      disk: 40
#      vcpus: 4

#For Nova-less provisioning enable the variable
novaless_prov: false
scale_compute_vms: false

new_nodes_instack: "{{ playbook_dir }}/newnodes.json"

# Enables composable_roles
# If lab_name not in [scale, alias] set undercloud_local_interface.
#Specify the controller ifaces for composable roles explicitly in
#case you need specific machines as controllers. Else default controller
#machine type is the first node in overcloud_instackenv.json
composable_roles: false
# controller_ifaces: []
# controller_machine_type: "1029p"
# ceph_ifaces: []
# ceph_machine_type:

#Ceph deployment
ceph_enabled: false
storage_node_disks: []
osd_scenario: lvm
osd_objectstore: bluestore
